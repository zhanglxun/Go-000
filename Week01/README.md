## Golang进阶

### 1. 概览与治理

#### 微服务

1. ownership 很重要～you build it, you fix it；
2. 小即是美，职责单一；
3. 今早定义原型和交互的API，前端可以mock出api 先进行开发；
4. 微服务定义：原子服务，独立进程，隔离部署，去中心化服务治理；
5. 基础设施建设，复杂度高；
6. 可测试性问题，问题排查复杂度提高； 

#### 相关设计

1. 电信机房，一个停机维护，几个小时；
2. 网络请求，for循环，请求下游的服务返回数据，会放大100倍；一定不能这样，需要用batch 批量的方式；
3. 首页很多内容聚合，不一定要并行化，可以分开异步化；
4. 分布式数据一致性的问题；
5. 测试比较困难，测试环境多测试环境，流量染色的玩法；
6. 日志服务的问题，早之前是是用ecs 直接上去 vim 看；微服务需要集中日志系统，服务太多；
7. 基建投入一个团队，专门做基建；
8. 组件服务化（ESB服务总线的传统系统的做法）；基础框架比如spring cloud ，go-micro，等；
9. service ：业务代码+ 基础库+ 第三方依赖；
10. 组织结构对微服务的影响；ownership：you build it, you fix it；全栈的思路；测试和运维，持续提供“好用”的：测试平台和套件，监控平台，发布平台，指标采集体系，日志监控平台；提供内部DevOps 的基础设施；
11. 信息沟通，信息同步。尽量比较熟悉和场景一致的团队，一致协作沟通；前端和网关Bff 沟通；
12. 去中心化，数据，治理 和技术区中心话；
13. 基础设计自动化；yapi可以看看。
14. desgin for failure,鲁棒性编程；粗粒度的进程间通讯；

> API gateway

1. app，web --  nginx -- 各种服务都对外：
   1. 客户端版本，里面写了各种对外的api，客户不升级的，版本无法收敛；app 里面，ngixn里面一堆的各种适配的代码；功能升级就非常多的问题要兼容，不敢动，强耦合后，根本无法做大规模的微服务重构；
   2. 后端说api这么清真，你前端为什么不能调的 7~8 个接口，把业务组合起来？移动端说，我组装这个，不科学啊，需要多次请求，工作量巨大啊。找接口，找人，维护成本非常高，要找好几个业务方联调；客户的调7~8个接口，延迟又高，还无法降级，一个接口问题，前端要做一堆处理，容错和判断。简直要炸了；
   3. app迭代速率要快，发个版不容易啊；重的工作 一定要在服务端做；做面向用户场景的组装；
   4. 面向端的适配（字段适配，设备流量适配2G网络，清晰度，请求埋点适配），非常痛苦啊，手机，pad，web，小程序等；要在后端服务里面耦合；
   5. 每个服务要做安全认证，限流；
2. app，web --  nginx-- BFF  -- 各种服务通过BFF聚合
   1. 协议一直，闭环，按业务场景设计api
   2. 根据终端 来进行适配；（可以根据设计，机型，网络速度，清晰度等适配），数据裁剪
   3. 动态升级方便，可以BFF 进行，沟通效率和升级；
3. app，web --  nginx-- BFF（多个）  -- 各种服务通过BFF聚合
   1. 第二代，有单点故障；
   2. 数据组装，业务集成度高，拆分成多个 BFF了
   3. 日志监控，权限，安全认证，限流 还是有很多重复的建设的问题；
   4. 一旦要升级基础库这些，还是需要全部的BFF 都更新，影响业务；
4. app，web --  nginx-- APi gateway-- BFF（多个）  -- 各种服务通
   1. 面向横切的，无业务逻辑的，都整合到api 网关里面；
   2. 包括 路由，认证，限流，安全等；
   3. 关注点分类的 思想，BFF 只关注业务数据的组装，数据编排；

#### 服务发现

1. B站 参考eureka 服务发现，自己做了个bilibili-discovery；
2. 平滑上线，平滑下线，health cheack 心跳检查；平滑上线有外挂模式，注册到注册中心，平滑下线，要先把流浪汉请求处理完；
3. 介绍了注册中心的healthCheck，以及使用 healthCheck ，来实现一些比较好玩的场景；
4. 服务器实例启动的时候，它的网络地址会写到注册表上，当服务器实例终止时，在从注册表删除；通过实例的心跳机制，动态刷新。客户端使用一个负载均衡算法，去选择 一个可用的服务实例，来响应这个请求。

####  多集群&多租户

1. 介绍了多租户和多集群的使用场景。一般采用N+2集群。
2. 多集群，各自独占缓存，主要是在生产环境，当一个集群出现故障，流量能被转移到其他的集群，用集群做隔离，可以提供系统整理的可用性；
3. 但是流量直接打过去，又会有热数据，冷数据的问题，比如做用户服务的例子，用户服务的热数据，会用缓存redis 进行cache。但是其他服务直接流量打过来，由于业务的不同，可能都是 cache miss，都是冷数据缓存没有命中，直接缓存穿透，会瞬间对数据库造成很大的压力；
4. 那么针对这种情况，介绍了业务隔离的情况下，通过缓存随机sharding 的方式，是缓存都变的热起来，避免 cache miss 导致缓存穿透的问题；
5. 而业务由于上述问题，也会由于全节点，全连接的原因，也有可能集群的HealthCheck 过高，有些情况出现风暴的情况，介绍了子集算法；从全集群中选取 一批节点(子集)，利用划分子集限制连接池 大小。
6. 多租户的理解：
7. 微服务架构中，运行系统共存，是利用微服务稳定性及模块化最有效的方式之一。
8. 可以是测试，金丝雀发布，影子系统(shadow systems)，甚至服务层或者产品线，使用租户能 够保证代码的隔离性并且能够基于流量租户做路由决策。
9. 并行测试在开发测试环境中，比较常见。工程师一次变动的环境发布，会导致测试环境的服务不可用，影响到其他团队；通过多环境的部署到测试环境，可以在不影响其他团队测试的情况下，进行分支测试；
10. 染色发布，可以把待测试的服务，隔离在一个沙盒环境，通常保持流量可以露营到指定服务，待测试服务仅处理测试流量而不处理生产流量。
11. 流量路由：能够基于流入栈中的流量类型，做路由；隔离性：能够可靠的隔离测试和生产中的资源，做全链路压测，这个牛批；
12. 具体的实现：给入站的请求绑定上下文（http header），在golang中，可以使用in-process 的context 传递；款服务调用，使用 metadata 。能够基于租户路由隔离流量。
13. 在微服务架构中典型的基 础组件是日志，指标，存储，消息队列，缓存以及配置。基于租户信息隔离数据需要分别处理基础组件。

### 2. 第二课笔记

